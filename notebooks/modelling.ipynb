{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a796d531",
   "metadata": {},
   "source": [
    "# Grant Classification\n",
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e890477",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29565df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import naive_bayes, svm, linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "import xgboost\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from keras import Sequential, layers\n",
    "from keras.backend import clear_session\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "np.random.seed(359)  # reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3de0c8",
   "metadata": {},
   "source": [
    "#### Getting the data ready:\n",
    "See data analysis notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c6cad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "de = spacy.load('de_core_news_sm')\n",
    "stopwords = de.Defaults.stop_words\n",
    "\n",
    "# Read data\n",
    "data = pd.read_excel(io=\"../data/data2020.xlsx\",\n",
    "                     usecols=['Politikbereich', 'Zweck'])\n",
    "\n",
    "# Clean data\n",
    "umlauts = {'ä': 'ae', 'ö': 'oe', 'ü': 'ue'}\n",
    "data['Zweck_clean'] = data['Zweck'].str.replace('[^\\w\\s]+', ' ', regex=True) \\\n",
    "                             .str.replace('\\d+', ' ', regex=True) \\\n",
    "                             .str.replace(' +', ' ', regex=True) \\\n",
    "                             .replace(umlauts, regex=True) \\\n",
    "                             .str.lower() \\\n",
    "                             .apply(lambda x: [word.lemma_ for word in de(str(x))]) \\\n",
    "                             .apply(lambda x: [item for item in x if item not in stopwords]) \\\n",
    "                             .str.join(' ')\n",
    "\n",
    "# Train/test split\n",
    "data = data.groupby('Politikbereich').filter(lambda x: len(x) > 1)\n",
    "train_X, test_X, train_y, test_y = train_test_split(data['Zweck_clean'], \n",
    "                                                    data['Politikbereich'], \n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=data['Politikbereich'])\n",
    "\n",
    "# Encoding y\n",
    "encoder = LabelEncoder()\n",
    "encoder = encoder.fit(data['Politikbereich'])\n",
    "train_y_enc = encoder.transform(train_y)\n",
    "test_y_enc = encoder.transform(test_y)\n",
    "\n",
    "# Vectorising x\n",
    "Tfidf_vect = TfidfVectorizer(max_features=500)  # TOD: play with that\n",
    "Tfidf_vect.fit(data['Zweck_clean'])\n",
    "train_X_tfidf = Tfidf_vect.transform(train_X)\n",
    "test_X_tfidf = Tfidf_vect.transform(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e15446e",
   "metadata": {},
   "source": [
    "### Modelling approaches\n",
    "\n",
    "We are first going to test several simpler classification approaches - Generalized Linear Model, Naive Bayes, Support Vector Machines and XGBoost.\n",
    "\n",
    "We are going to fit all of them without any tuning and then tune them as to maximize performance.\n",
    "\n",
    "We are going to compare them on accuracy, f1 micro, area under ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1aa3be",
   "metadata": {},
   "source": [
    "#### Logistic Regression, Naive Bayes and SVMs\n",
    "\n",
    "We are going to use these models as a baseline for the more complex tuned models we are going to try to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66f005c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6308784383318545\n"
     ]
    }
   ],
   "source": [
    "LG = linear_model.LogisticRegression(max_iter=200)\n",
    "LG.fit(train_X_tfidf, train_y_enc)\n",
    "\n",
    "predictions_LG = LG.predict(test_X_tfidf)\n",
    "\n",
    "print(accuracy_score(predictions_LG, test_y_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "897ab2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6069210292812778\n"
     ]
    }
   ],
   "source": [
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(train_X_tfidf, train_y_enc)\n",
    "\n",
    "predictions_NB = Naive.predict(test_X_tfidf)\n",
    "\n",
    "print(accuracy_score(predictions_NB, test_y_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "380aa99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6339840283939663\n"
     ]
    }
   ],
   "source": [
    "SVM = svm.SVC(C=1, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(train_X_tfidf, train_y_enc)\n",
    "\n",
    "predictions_SVM = SVM.predict(test_X_tfidf)\n",
    "\n",
    "print(accuracy_score(predictions_SVM, test_y_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b57f2",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed012903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tuning: 0.6353149955634427\n"
     ]
    }
   ],
   "source": [
    "xg = xgboost.XGBClassifier(use_label_encoder=False, eval_metric=\"rmse\")\n",
    "\n",
    "xg.fit(train_X_tfidf, train_y_enc)\n",
    "predictions_xg = xg.predict(test_X_tfidf)\n",
    "print(\"No tuning:\", accuracy_score(predictions_xg, test_y_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e30d38",
   "metadata": {},
   "source": [
    "#### Tuning XGBoost using Grid Search\n",
    "We choose the parameters we want to test. The size and number of trees are always a parameter of interest. We also want to add a few more to try to reduce overfitting - by not allowing splits if there are too many samples or by not using all data in a given boosting iteration for example.\n",
    "\n",
    "We will furthermore try using the mean absolute error as an evaluation metric. A prediction is that there may be a lot of outliers (for example single words that aren't actually connected to the topic and are there because of other/random factors - such as names). The mean absolute error penalizes those outliers more (because it doesn't squre their error). As done in different variations of Robust Linear Regression, this might give a boost to the model and should be examined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31ef8a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_child_weight': [1, 10],\n",
    "        'gamma': [0.5, 1, 2],\n",
    "        'subsample': [0.7, 1.0],\n",
    "        'eval_metric': [\"mae\", 'rmse'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c008183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boris/code/anaconda3/envs/grant-clf/lib/python3.10/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned model accuracy: 0.6353149955634427\n",
      "Parameters used: {'eval_metric': 'mae', 'gamma': 0.5, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "xg = xgboost.XGBClassifier(use_label_encoder=False)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3, shuffle = True, random_state = 359)\n",
    "grid = GridSearchCV(estimator=xg, param_grid=params, scoring='f1_micro', n_jobs=8, cv=skf.split(train_X_tfidf,train_y_enc))\n",
    "\n",
    "grid.fit(train_X_tfidf, train_y_enc)\n",
    "predictions_xg_tuned = grid.best_estimator_.predict(test_X_tfidf)\n",
    "print(\"Tuned model accuracy:\", accuracy_score(predictions_xg_tuned, test_y_enc))\n",
    "print(\"Parameters used:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2c2f4b",
   "metadata": {},
   "source": [
    "Parameters used: {'eval_metric': 'mae', 'gamma': 0.5, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f7e9dd",
   "metadata": {},
   "source": [
    "#### Evaluation of Configuration\n",
    "\n",
    "Selecting a baseline model\n",
    "\n",
    "We are going to use the following baselines:\n",
    "1. Always predicting the most populous class (ca 25% accuracy)\n",
    "2. Using a generalized linear model (ca 60% accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c02a08d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a20c77a",
   "metadata": {},
   "source": [
    "#### UNFINISHED: CNN\n",
    "\n",
    "We are going to train a convolutional neural network to try to beat the other approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "67a487ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 200)               100200    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 29)                5829      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 106,029\n",
      "Trainable params: 106,029\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "clear_session()\n",
    "\n",
    "train_y_k = tf.keras.utils.to_categorical(train_y_enc)\n",
    "test_y_k = tf.keras.utils.to_categorical(test_y_enc)\n",
    "\n",
    "model = Sequential()\n",
    "input_dim = train_X_tfidf.shape[1]\n",
    "\n",
    "#model.add(layers.Embedding(input_dim=input_dim, \n",
    "#                           output_dim=200, \n",
    "#                           input_length=500))\n",
    "#model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "#model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(200, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(29, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ed821910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.6989\n",
      "Testing Accuracy:  0.6362\n"
     ]
    }
   ],
   "source": [
    "clear_session()\n",
    "\n",
    "model.fit(train_X_tfidf.toarray(), train_y_k,\n",
    "          epochs=10,\n",
    "          verbose=False,\n",
    "          validation_data=(test_X_tfidf.toarray(), test_y_k),\n",
    "          batch_size=100)\n",
    "loss, accuracy = model.evaluate(train_X_tfidf.toarray(), train_y_k, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(test_X_tfidf.toarray(), test_y_k, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c477f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ed992b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grant-clf",
   "language": "python",
   "name": "grant-clf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
